{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67de738",
   "metadata": {},
   "source": [
    "# Iterables and generator functions\n",
    "\n",
    "## Iterables\n",
    "\n",
    "Iterables: object that can be iterated (for example a list, dict, set, tuples), under the hood iterables have a `__iter__` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedac112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example (thanks chatGPT)\n",
    "class MyIterable:\n",
    "    def __init__(self, start):\n",
    "        self.start = start\n",
    "\n",
    "    # this function returns the iterable (it returns 'self')\n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    # this functions is called to 'iterate' through it\n",
    "    def __next__(self):\n",
    "        if self.index >= self.start:\n",
    "            raise StopIteration\n",
    "        # counting the numbers down\n",
    "        value = self.start - self.index\n",
    "        self.index += 1\n",
    "        return value\n",
    "\n",
    "    # support for length \n",
    "    def __len__(self):\n",
    "        return self.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new MyIterable, set 'start' to 5\n",
    "c = MyIterable(5)\n",
    "\n",
    "# length of iterable means calling __len__\n",
    "print('length', len(c))\n",
    "\n",
    "# iterate\n",
    "for num in c:\n",
    "    print('Number is', num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd43aa4",
   "metadata": {},
   "source": [
    "## Generator functions\n",
    "\n",
    "Generator functions are functions that 'yield' one result from an iterable (instead of returning everything in one go). \n",
    "We use these function when processing many (large) files. Instead of reading all files into memory, and then processing each file one by one, a generator function reads one file at the time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed389f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    # instead, we read 100,000 files of 20MB each\n",
    "    for i in [1, 5, 11, 22]:\n",
    "        print('about to yield', i)\n",
    "        # return one file of 20MB\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef995ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new generator \n",
    "my_gen = create_generator()\n",
    "my_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it doesn't know its length\n",
    "len(my_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in my_gen:\n",
    "    print('just asked for a number ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e72eec",
   "metadata": {},
   "source": [
    "### Combined: Generator function as iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "import gensim\n",
    "import os, string, re, glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english') )\n",
    "\n",
    "class MyDocuments():\n",
    "\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.files = glob.glob('{}/*.txt'.format(folder))\n",
    "            \n",
    "    def __len__ (self):\n",
    "        return (len(self.files))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            with open( file, encoding='utf-8') as f:\n",
    "                content = f.read() \n",
    "            # tokenize\n",
    "            file_tokens = [x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]            \n",
    "            yield file_tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable that returns one document at the time\n",
    "fileList = MyDocuments(r'C:\\Users\\joost\\Documents\\teaching\\acg7849-python\\comment_letters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of files\n",
    "len(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through tokenized_files (one at the time) \n",
    "# it is not subscriptable though\n",
    "counter = 0\n",
    "for f in fileList:\n",
    "    # f is a tokenized document, show first 10 words\n",
    "    print (counter, f[0:10])\n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ce53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
