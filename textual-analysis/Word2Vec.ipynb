{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Example - Comment letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on: https://rare-technologies.com/word2vec-tutorial/\n",
    "# further reading: Corpora and Vector Spaces - https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, string, glob, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english') )\n",
    "\n",
    "# add some punctuation to string.punctuation\n",
    "punc = string.punctuation + '“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do: pip install w3lib first\n",
    "from w3lib.html import replace_entities\n",
    "\n",
    "# function that converts html to text\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>', flags=re.DOTALL)\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return replace_entities(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "class myCommentLetters(object):\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    " \n",
    "    def __iter__(self):\n",
    "        counter = 0\n",
    "        # I usually read only a portion of the files when debugging        \n",
    "        #for f in glob.glob('{}/*.txt'.format(self.folder))[0:100]:        \n",
    "        # process all files\n",
    "        for f in glob.glob('{}/*.txt'.format(self.folder)):  \n",
    "            \n",
    "            # read and tokenize file\n",
    "            with open( f, encoding='utf-8') as f:\n",
    "                content = cleanhtml( f.read() )\n",
    "            # to have an idea of where we are\n",
    "            counter += 1\n",
    "            if counter % 50 == 0:\n",
    "                print('counter', counter)\n",
    "            # remove numbers, see https://stackoverflow.com/questions/57030670/how-to-remove-punctuation-and-numbers-during-tweettokenizer-step-in-nlp\n",
    "            content = re.sub(r'\\d+', '', content)\n",
    "            # yield ('return') tokenized file\n",
    "            yield [x.lower() for x in word_tokenize(content) if x.lower() not in stopWords and x not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a memory-friendly iterator\n",
    "wordLists = myCommentLetters(r'C:\\Users\\joost\\Documents\\teaching\\acg7849-python\\comment_letters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# note how worLists is passed as an argument\n",
    "# This is typical for neural network algorithms: these expect something that is iterable\n",
    "# can we pass in a list of all files? Sure\n",
    "# But what if the files don't fit in memory? -> generator function that yields one file at a time\n",
    "# min-count is how minimum count of each word, default is 5\n",
    "# workers: #cores (may need cython to be installed for this to have an effect)\n",
    "model = gensim.models.Word2Vec(wordLists, min_count=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw NumPy vector of a word (needs to be in model)\n",
    "# note we made all words lowercase\n",
    "model.wv['dear']                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try similarity of: tax, liability, comment, deduction\n",
    "model.wv.most_similar('accrual', topn=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which does not belong? (note: each of these should be in the model)\n",
    "model.wv.doesnt_match(\"warranty liability claim contingency tax\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
